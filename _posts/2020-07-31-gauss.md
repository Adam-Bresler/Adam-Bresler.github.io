---
title: "Estimating parameters using the method of least squares and the Gauss-Seidel algorithm"
date: 2020-07-20
tags: [data science, parameter estimation, algorithms]
excerpt: "Data Science, Parameter Estimation, Algorithms"
mathjax: "true"
---

This post was adapted from an assignment completed for the statistical computing module as part of
the statistic honours course at the University of Cape Town. We were given data drawn from:

$$y = a_1\exp\left(\frac{-x}{a_2}\right) + a_3x\exp \left(\frac{-x}{a_4}\right) $$

We need to estimate $$a_1, a_2, a_3$$ and $$a_4$$. In the plot below, it is clear that this will be difficult to do by guessing,
thus we use the method of least squares and a Gauss-Seidel algorithm to do this.

<img src="{{ site.url }}{{ site.baseurl }}/images/gauss/inital_data.png" alt="given data">


However, what actually do these methods entail? In essence, the method of least squares estimates
the best fitting parameters by minimising the sum of the square difference between the estimated
points and the actual data, or $$min\left(\sum(y - \hat{y})^2\right)$$. For any given parameters,
we can then see how 'far off' our data is compared to our estimate. However, we of course need
to minimise and in order to do this, we need to use a Gauss-Seidel algorithm.

The Gauss-Seidel algorithm is an iterative method that updates each parameter while using current estimates
for the others. In each iteration, the algorithm will find the lowest residual value by alternating
one parameter and holding the rest constant. It then continues this process for all parameters in a loop. We do
this until all parameters converge to a single value, stopping when the difference between the old estimate and
the new one is very small. One issue with the Gauss-Seidel estimate is that it cannot distinguish between local and
global minima. Thus, we need to trial many different starting points until we find the true global minimum.

The code to implement this is pretty simple:

```r
f.hat <- function(a1,a2,a3,a4){
#It is better to accept x and y as a parameter so it is reusuable.
#I decided not to for speed and because it was only used in one question

  y.hat <- a1*exp((-x)/a2) + a3*x*exp((-x)/a4)
  res <- sum((y-y.hat)^2)
  return(res)
}

# Set starting point
a1 <- 1
a2 <- 2
a3 <- 3
a4 <- 4
eps <- 0.0001

i = 1
a1.save <- c() #Vectors to save guesses, not necessary but nice to check
a2.save <- c()
a3.save <- c()
a4.save <- c()

# Gauss Seidel Loop

repeat{
  a1.old <- a1
  a2.old <- a2
  a3.old <- a3
  a4.old <- a4

  a1 <- optimise(f.hat, c(-100, 100), a2 = a2, a3 = a3, a4 = a4)$minimum #Optimise a1 holding rest at current estimate
  a2 <- optimise(f.hat, c(-100, 100), a1 = a1, a3 = a3, a4 = a4)$minimum
  a3 <- optimise(f.hat, c(-100, 100), a1 = a1, a2 = a2, a4 = a4)$minimum
  a4 <- optimise(f.hat, c(-100, 100), a1 = a1, a2 = a2, a3 = a3)$minimum


  if(abs(a1 - a1.old) < eps & abs(a2 - a2.old) < eps & abs(a3 - a3.old) < eps & abs(a4 - a4.old) < eps){
    break #Break if all parameters converge
  }

 a1.save[i] <- a1
 a2.save[i] <- a2
 a3.save[i] <- a3
 a4.save[i] <- a4
 i = i + 1
}
```

This approach gives us parameter estimates of a1 = 3.993, a2 = 2.01, a3 = 0.998 and a4 = 10.046. If we plot this against our observed data,
we can see our estimated function fits well.

<img src="{{ site.url }}{{ site.baseurl }}/images/gauss/gauss.png" alt="algorithm results">

Finally, we can check these results against those given by the built in function of R, nlm(),
which minimises any given function.

```r
f.hat2 <- function(par){
#We define exactly the same function that accepts a vector of length p to use in nlm
#It is identical in the way it works but allows for the vector argument

  a1 = par[1]
  a2 = par[2]
  a3 = par[3]
  a4 = par[4]

  y.hat2 <- a1*exp((-x)/a2) + a3*x*exp((-x)/a4)
  res2 <- sum((y-y.hat2)^2)
  return(res2)
}

#Test values from built in functions

nlm <- nlm(f.hat2, c(1,2,3,4))
optim <- optim(c(1,2,3,4), f.hat2)
```

This gives us the estimates of a1 = 3.994, a2 = 2.008, a3 = 0.999 and a4 = 10.044.
We can see that our estimated values are incredibly close to these estimates, and we
can thus conclude they are the most likely values that generated our observed data.
Finally, we can check how these look compared to our estimates through a graph.

<img src="{{ site.url }}{{ site.baseurl }}/images/gauss/nlm.png" alt="algorithm results">

### Conclusions

Through this process we have shown that there are many methods to estimate parameters, and when done correctly,
these produce similar results. We can say our data was generated from the model:

$$y = (3.993)\exp\left(\frac{-x}{( 2.01)}\right) + (0.998)x\exp \left(\frac{-x}{(10.046)}\right) $$
While these parameters may slightly differ from method to method, they differ by minute amounts and we
can be confident in these parameter estimates.

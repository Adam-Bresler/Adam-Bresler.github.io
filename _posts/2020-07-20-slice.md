---
title: "Slice Sampling"
date: 2020-07-20
tags: [bayesian statistics, data science]
excerpt: "Bayesian Statistics, Data Science"
mathjax: "true"
---

*This post was adapted from an assignment completed for the bayesian statistics module as part of the statistic honours course at the University of Cape Town. It details the Markov Chain Monte Carlo (MCMC) method Slice Sampling, however does not explain MCMC alogrithms as a whole. It also assumes background knowledge in bayesian statistics.*

Slice sampling allows us to draw samples from a target distribution, similar to other MCMC methods. In this method, a distribution is sampled by taking a uniform random sample from the region under the plot of the density function.

Slice sampling has one big advantage: there is no need for a proposal distribution. We simply need our target distribution, or some $$f(x)$$ proportional to it, in order to sample. Slice sampling also has the added advantage of automatically resolving step size. This is controlled by the variance in Metropolis-Hastings, and removes the needed to tune this parameter.

However, there are a few downsides too. Firstly, the algorithm is a slower than Metropolis-Hastings and this needs to be considered if short on computing power. As will be expanded on later, it is also difficult to find the intersection point between the horizontal slice and the target distribution. Luckily, this can be combated through built-in R functions in some cases.

## The Algorithm
This leads us to asking, how does slice sampling actually work? The slice sampling algorithm is easiest to understand in the univariate case. The multivariate case is simply an extension of this, sampling for each variable in turn. Thus, we will only explore the univariate case as it is easy to extend to the multivariate, and all understanding is gained in univariate examples.

Slice sampling follows a set of basic steps:


1. Choose a random starting value of x, let's call this  $$x_0$$, $$f(x_0) > 0$$
2. Uniformly sample a value for y on the interval $$\left[0, f(x_0)\right]$$
3. Draw the horizontal line across the curve at this y, see figure below
4. Find all line segments under the curve
5. From the slices that fall under the curve, sample a random point $$(x_1,y)$$
6. Repeat from step 2, using this new x.


Step four is where this is tricky, as it involves finding which segments lie under the curve. For any distribution, this can be achieved by using a stepping out procedure, or a doubling procedure.

<img src="{{ site.url }}{{ site.baseurl }}/images/slice/Slice.png" alt="slice sampling under the curve">

### Step-out Procedure
The step-out procedure involves selecting an interval of size $$w$$ around $$x_0$$. The width parameter $$w$$ is one of the few tuning parameters needed. It does not have to be exact, but should be around the average width of a segment that is under the curve. Once $$w$$ has been applied, both end-points are checked to test whether they are under the curve or not. If not, the end point is expanded by length $$w$$ until both end points of the segment lie outside the curve. This is where we define our second tuning parameter, $$m$$. This is an integer that limits the size of each slice, where $$m\times w$$ is the maximum width.

To understand this, we will look at some pseudo-code. We define a function that performs this step-out procedure, creating the interval that contains all the points under the curve.

```r
segment = function(x_0, y, w, m){
  #y is our uniform draw from 0 to f(x)

  L = x_0 - runif(1,0,w)
  R = L + w

  # J and K are used to break the loop if bounds get too big
  J = floor(m * runif(1))
  K = (m - 1) - J

  #Target is our f, defined elsewhere

  while ((y < target(L)) & J > 0){
    L = L - w
    J = J - 1
    }

  while ((y < target(R)) & K > 0){
    R = R + w
    K = K - 1
    }


  return(list(L=L,R=R))
}
```

This algorithm is a direct interpretation of that found in and defines in R a working solution to the slice sampling method.

### Doubling
The step-out procedure is not the only method available to us in order to create intervals. We can also use the doubling technique. This is similar to the step-out procedure. In this method, we double the size of each interval until both end points are again outside the curve. This is more efficient when our $$w$$ is small, and finds the desired interval much faster. The disadvantage however is that both sides are not expanded equally, and can lead to heavily skewed intervals.

Instead of defining $$m$$ as our limiting factor, we now instead define $$p$$ where $$2^p\times w$$ is the maximum slice size. Again, we can see how this is implemented below:

```r
segment_doubling <- function(x_0, y, p, w){

  L = x_0 - runif(1, 0, w)
  R = L + w
  K = p

  while ((y < target(L) | y < target(R)) & K > 0){
    if (runif(1) < 0.5){
      L = L - (R - L)
    }

    else{
    R = R + (R - L)
    }

    K = K - 1
  }
}
```

## Sampling
Now that we have defined these different methods of finding the interval below the curve, we can proceed to the objective of the algorithm, sampling from a target distribution.

As a reminder to the process, we begin by defining a starting value for x. We then draw a random number, y, between 0 and $$f(x)$$. We then draw a horizontal line at this y across the curve. We then do the segmenting procedure above to find an interval that lies under the curve.

The interval we find is uniformly sampled to find our $$x_1$$. If $$x_1$$ is still outside the curve, it becomes the new end-point, and we repeat this until $$x_1$$ lies within the segment. While this is not strictly needed, this is done in order to boost the efficiency of the algorithm. If using the step-out procedure, this is called shrinking. It is easy to implement, and we can see this in the code below:

```r
shrink_step = function(x_0, y, int){
  L = int$L
  R = int$R

  repeat{
    x_1 = runif(1, L, R)

    if (y < target(x_1)){
      return(x_1)
    }

    #If bigger, make upper bound
    if (x_1 > x_0){
      R = x_1
    }

    #If smaller, lower bound
    if(x_1 < x_0){
    L = theta_prop
    }

  }
}
```

This becomes a little more complicated when using the doubling procedure. This now works by cycling back through the intervals we have doubled. Thus, we first check whether the new $$x_1$$ is acceptable or not. We say it is acceptable when the intervals generated from the new point are either identical to the old, or these immediate intervals do not have both endpoints outside the interval. First, we define a function that checks for acceptance:

```r
accept = function(x_0, x_1, L, R, y, w){
  D = FALSE
  #1.1 stops rounding errors
  while (R - L > 1.1 * w){
    M = (L + R)/2

    if ((x_0 < M & x_1 >= M) | (x_0 >= M & x_1 < M)){
      D = TRUE
    }

    if (x_1 < M){
      R = M
    }

    else{
      L = M
    }

    if (D & y >= target(L) & y >= target(R)){
      return(FALSE)
    }

  }
  return(TRUE)
}
```

Now, we simply call this during our slice procedure, repeating until we accept a new $$x_1$$

```r
shrink_doubling = function(x_0, y, int){
  L = int$L
  R = int$R

  repeat{
    x_1 = runif(1, L, R)
    if (y < target(x_1) & accept(x_0, x_1, L, R, y, w)){
      return(x_1)
    }

    if (x_1 > x_0){
      R = x_1
    }

    if (x_1 < x_0){
      L = x_1}
  }

}
```

After this shrinking procedure and we have our $$x_1$$ for the corresponding y, we store this point as our random sample. We then introduce the Markov Chain portion of our algorithm, and use $$x_1$$ to find our next random y value by repeating the above.

The graphic, below, that illustrates this sampling process well. While it doesn't show the interval, we can get a graphical understanding of this sampling.

<img src="{{ site.url }}{{ site.baseurl }}/images/slice/slice_illustration.png" alt="slice sampling illustration">

## Implementation
Now that we have an in-depth understanding of how the algorithm works, we can undertake the main reason for doing this: to sample from a target distribution. In this example we will be sampling from a bimodal posterior distribution. While we will be using a know distribution in our implementation, this can be done using any posterior with known or unknown parameters.

We will be implementing slice sampling on a truncated normal distribution, which joins two normal distributions with equal weighting. This takes the form:

$$\pi = \frac{1}{2}N(-4, 1) + \frac{1}{2}N(4, 1)$$

First, we will use step out method and the functions we have already created. To do this, we simply define our final algorithm that will use the functions to perform slice sampling. This simply implements the procedure we have outlined, but using these functions to find our intervals and sample.

```r
slice_step = function(n, x_0, w, m){
  y = x = rep(NA, n)
  x[1] = x_0

  for (i in 2:n){
    y[i] = runif(1, 0, target(x[i - 1]))
    int = segment(x[i-1], y = y[i], w, m)

    x[i] = shrink_step(x[i-1], y[i], int)}

  return(data.frame(x, y))
  }
```
If we plot our results, we see that this method has closely sampled our posterior distribution as we expected.

<img src="{{ site.url }}{{ site.baseurl }}/images/slice/step_done.png" alt="step illustration">

The step-out procedure has worked incredibly well in sampling from our posterior distribution! Now, we will attempt the same using the doubling procedure and plot. We again define a function that calls the previously defined functions in order to perform the slice sampling:

```r
slice_doubling = function(n, x_0, w, p){
  y = x = rep(NA, n)
  x[1] = x_0

  for(i in 2:n){
  y[i] = runif(1, 0, target(x[i - 1]))
  int <- segment_doubling(x[i-1], y[i], w, p)

  x[i] <- shrink_doubling(x[i-1], y[i], int)

}
  return(data.frame(x, y))
}
```

<img src="{{ site.url }}{{ site.baseurl }}/images/slice/doubling_done.png" alt="doubling illustration">

We can see the doubling slice sampling procedure also samples incredibly well from our posterior distribution. Thus, we can conclude that we have successfully implemented the slice sampling algorithm using both the step-out procedure and the doubling procedure.

## The Multivariate Case
As was briefly mentioned before, now that we have an in-depth understanding of the slice-sampling procedure, it is easy to extend this to the multivariate case. In order to do this, we treat each variable independently. This is exactly the same process that is implemented in other Markov Chain Monte Carlo algorithms, such as Gibbs sampling. We update each variable in-turn, holding the others constant as we sample from their posteriors. We can also use other slice sampling methods that are specific to the multivariate case, such as the reflective slice-sampler or elliptical slice sampler.


## Final Notes
In the code the parameter is defined as 'x'. While it does give us our x coordinate, in reality we are sampling the dependent parameter in the posterior. Thus, our posterior takes the form $$\pi(x|y)$$ however this is flexible to any parameter for any posterior and is just a matter of notation.
